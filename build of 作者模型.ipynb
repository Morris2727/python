{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator         #To increase the number of data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GRU, Embedding, Flatten, concatenate, Input\n",
    "#Dense for NN, Conv2d Maxpool2d Flatten for CNN , GRU Embedding for RNN , Input concatenate for functional API\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D, Reshape\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width, image_height = 114,114 \n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, image_width, image_height)\n",
    "else:\n",
    "    input_shape = (image_width, image_height, 1)\n",
    "    \n",
    "train_dir = r'C:\\Users\\morri\\OneDrive\\桌面\\naruto-handsigns-predict-dl-master\\datasetTrain'                #input-to train\n",
    "validation_dir = r'C:\\Users\\morri\\OneDrive\\桌面\\naruto-handsigns-predict-dl-master\\datasetTest'            #input-to test\n",
    "save_dir=r'C:\\Users\\morri\\OneDrive\\桌面\\naruto-handsigns-predict-dl-master\\save'      #Data generated by ImageDataGenerator\n",
    "\n",
    "train_samples = 5830                        #input\n",
    "validation_samples = 5830                    #input\n",
    "\n",
    "batch_size = 400\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9))                             #input\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8160 images belonging to 9 classes.\n",
      "Found 8160 images belonging to 9 classes.\n",
      "WARNING:tensorflow:From <ipython-input-5-8bbb88fa78c0>:13: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 14.575 steps, validate for 14.575 steps\n",
      "Epoch 1/25\n",
      "15/14 [==============================] - 109s 7s/step - loss: 64.0063 - accuracy: 0.1285 - val_loss: 2.2428 - val_accuracy: 0.1155\n",
      "Epoch 2/25\n",
      "15/14 [==============================] - 87s 6s/step - loss: 2.1810 - accuracy: 0.1250 - val_loss: 2.2046 - val_accuracy: 0.1220\n",
      "Epoch 3/25\n",
      "15/14 [==============================] - 101s 7s/step - loss: 2.1608 - accuracy: 0.1464 - val_loss: 2.1799 - val_accuracy: 0.1435\n",
      "Epoch 4/25\n",
      "15/14 [==============================] - 90s 6s/step - loss: 2.1517 - accuracy: 0.1592 - val_loss: 2.1507 - val_accuracy: 0.1863\n",
      "Epoch 5/25\n",
      "15/14 [==============================] - 79s 5s/step - loss: 2.1380 - accuracy: 0.1740 - val_loss: 2.1219 - val_accuracy: 0.1872\n",
      "Epoch 6/25\n",
      "15/14 [==============================] - 81s 5s/step - loss: 2.1163 - accuracy: 0.1825 - val_loss: 2.1143 - val_accuracy: 0.1705\n",
      "Epoch 7/25\n",
      "15/14 [==============================] - 86s 6s/step - loss: 2.0897 - accuracy: 0.1839 - val_loss: 2.1162 - val_accuracy: 0.1413\n",
      "Epoch 8/25\n",
      "15/14 [==============================] - 390s 26s/step - loss: 2.0841 - accuracy: 0.1828 - val_loss: 2.1499 - val_accuracy: 0.1305\n",
      "Epoch 9/25\n",
      "15/14 [==============================] - 90s 6s/step - loss: 2.0437 - accuracy: 0.2048 - val_loss: 2.1050 - val_accuracy: 0.1502\n",
      "Epoch 10/25\n",
      "15/14 [==============================] - 87s 6s/step - loss: 2.0177 - accuracy: 0.2168 - val_loss: 1.9903 - val_accuracy: 0.2328\n",
      "Epoch 11/25\n",
      "15/14 [==============================] - 394s 26s/step - loss: 1.9674 - accuracy: 0.2441 - val_loss: 1.9676 - val_accuracy: 0.2413\n",
      "Epoch 12/25\n",
      "15/14 [==============================] - 85s 6s/step - loss: 1.9137 - accuracy: 0.2696 - val_loss: 1.7555 - val_accuracy: 0.4227\n",
      "Epoch 13/25\n",
      "15/14 [==============================] - 84s 6s/step - loss: 1.8711 - accuracy: 0.2917 - val_loss: 1.5823 - val_accuracy: 0.4697\n",
      "Epoch 14/25\n",
      "15/14 [==============================] - 99s 7s/step - loss: 1.8245 - accuracy: 0.2981 - val_loss: 1.8079 - val_accuracy: 0.3715\n",
      "Epoch 15/25\n",
      "15/14 [==============================] - 100s 7s/step - loss: 1.7962 - accuracy: 0.3274 - val_loss: 1.8520 - val_accuracy: 0.3257\n",
      "Epoch 16/25\n",
      "15/14 [==============================] - 88s 6s/step - loss: 1.7432 - accuracy: 0.3392 - val_loss: 2.5309 - val_accuracy: 0.2680\n",
      "Epoch 17/25\n",
      "15/14 [==============================] - 101s 7s/step - loss: 1.7320 - accuracy: 0.3473 - val_loss: 1.8095 - val_accuracy: 0.4203\n",
      "Epoch 18/25\n",
      "15/14 [==============================] - 93s 6s/step - loss: 1.6839 - accuracy: 0.3694 - val_loss: 1.4142 - val_accuracy: 0.5443\n",
      "Epoch 19/25\n",
      "15/14 [==============================] - 322s 21s/step - loss: 1.6687 - accuracy: 0.3753 - val_loss: 2.3547 - val_accuracy: 0.3073\n",
      "Epoch 20/25\n",
      "15/14 [==============================] - 98s 7s/step - loss: 1.5874 - accuracy: 0.4057 - val_loss: 1.7229 - val_accuracy: 0.4740\n",
      "Epoch 21/25\n",
      "15/14 [==============================] - 94s 6s/step - loss: 1.5950 - accuracy: 0.4003 - val_loss: 1.5810 - val_accuracy: 0.4603\n",
      "Epoch 22/25\n",
      "15/14 [==============================] - 431s 29s/step - loss: 1.5657 - accuracy: 0.4165 - val_loss: 1.9860 - val_accuracy: 0.4113\n",
      "Epoch 23/25\n",
      "15/14 [==============================] - 91s 6s/step - loss: 1.5313 - accuracy: 0.4327 - val_loss: 2.1159 - val_accuracy: 0.4257\n",
      "Epoch 24/25\n",
      "15/14 [==============================] - 86s 6s/step - loss: 1.5028 - accuracy: 0.4502 - val_loss: 2.6194 - val_accuracy: 0.3377\n",
      "Epoch 25/25\n",
      "15/14 [==============================] - 93s 6s/step - loss: 1.4718 - accuracy: 0.4510 - val_loss: 1.4836 - val_accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "###利用ImageDataGenerator生成一個數據生成器,word中有解釋\n",
    "###使用的參數有zoom_range, horizontal_flip,vertical_flip,rotation_range,shear_range和 width_shift_range & height_shift_range,詳細的介紹在word\n",
    "train_datagen = ImageDataGenerator(zoom_range=0.0, horizontal_flip=True,vertical_flip=True,rotation_range=30,shear_range=0.5,width_shift_range=0.5,height_shift_range=0.5)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(image_width, image_height), batch_size=batch_size, \n",
    "                                                    color_mode=\"grayscale\", class_mode='categorical',save_to_dir=save_dir)\n",
    "test_datagen = ImageDataGenerator()\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(image_width, image_height), batch_size=batch_size, \n",
    "                                                        color_mode=\"grayscale\", class_mode='categorical',save_to_dir=save_dir)\n",
    "\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch= train_samples / batch_size, epochs=epochs, validation_data=validation_generator, \n",
    "                    validation_steps= validation_samples / batch_size)\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_in_json.json\", \"w\") as json_file:\n",
    "    json.dump(model_json, json_file)\n",
    "\n",
    "\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
